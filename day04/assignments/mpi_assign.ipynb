{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9GqdeI_c2VL_",
   "metadata": {
    "id": "9GqdeI_c2VL_"
   },
   "source": [
    "## Exercise 1 Hello World\n",
    "\n",
    "1. Write an MPI program displaying the number of processes used for the execution and the rank of each process.\n",
    "2. Test the programs obtained with different numbers of threads for the parallel program.\n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "Hello from the rank 2 process\n",
    "Hello from the rank 0 process\n",
    "Hello from the rank 3 process\n",
    "Hello from the rank 1 process\n",
    "Parallel execution of hello_world with 4 process\n",
    "```\n",
    "*Note that the output order maybe different*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suCzN5P14FCK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suCzN5P14FCK",
    "outputId": "3a34bf9a-4993-436f-f578-5b72362c0515"
   },
   "outputs": [],
   "source": [
    "# !pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "buCEn20Z2VME",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buCEn20Z2VME",
    "outputId": "4d76dc7e-1520-4a06-f7c2-a8f9fcaa9e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello.py\n"
     ]
    }
   ],
   "source": [
    "%%file hello.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "print()\n",
    "print(\"Hello from the rank {rank} process\".format(rank=RANK))\n",
    "COMM.Barrier()\n",
    "if RANK == SIZE-1:\n",
    "    print(\"Parallel execution of hello_world with {} process\".format(SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lnX6_rAE2VMG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnX6_rAE2VMG",
    "outputId": "9757ff94-ed2c-4168-e654-1be33c62e95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key\n",
      "Hello from the rank 2 process\n",
      "\n",
      "Hello from the rank 3 process\n",
      "\n",
      "Hello from the rank 0 process\n",
      "\n",
      "Hello from the rank 1 process\n",
      "Parallel execution of hello_world with 4 process\n"
     ]
    }
   ],
   "source": [
    "# enter command for compile and run the program\n",
    "! mpirun -n 4 python hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ci-mObAi2VMH",
   "metadata": {
    "id": "ci-mObAi2VMH"
   },
   "source": [
    "## Exercise 2 Sharing Data \n",
    "\n",
    "A common need is for one process to get data from the user, either by reading from the terminal or command line arguments, and then to distribute this information to all other processors.\n",
    "\n",
    "Write a program that reads an integer value from the terminal and distributes the value to all of the MPI processes. Each process should print out its rank and the value it received. Values should be read until a negative integer is given as input.\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Get_rank` `Bcast` \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "10\n",
    "Process 0 got 10\n",
    "Process 1 got 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "KVbSOoRy2VMI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVbSOoRy2VMI",
    "outputId": "965b1a34-8a37-4b1e-c8ff-b4850c6b4a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sharing.py\n"
     ]
    }
   ],
   "source": [
    "%%file sharing.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "recv = 1\n",
    "\n",
    "while recv > 0:\n",
    "    if RANK == 0 :\n",
    "        send = int(input()) \n",
    "    else: \n",
    "        send = None\n",
    "    \n",
    "    recv = COMM.bcast(send, root=0)\n",
    "    print(\"Process {rank} got {recv} \".format(rank=RANK, recv=recv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "C0X6LpopxmM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0X6LpopxmM0",
    "outputId": "69615f72-e2ab-4931-b05f-79a7b70c448b"
   },
   "outputs": [],
   "source": [
    "# enter command for compile and run the program\n",
    "# ! mpirun -n 2  python3 sharing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_3Sur7t62VMJ",
   "metadata": {
    "id": "_3Sur7t62VMJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "x6yIoGvX2VMJ",
   "metadata": {
    "id": "x6yIoGvX2VMJ"
   },
   "source": [
    "## Exercise 3 Sending in a ring (broadcast by ring)\n",
    "\n",
    "Write a program that takes data from process zero and sends it to all of the other processes by sending it in a ring. That is, process i should receive the data and send it to process i+1, until the last process is reached.\n",
    "Assume that the data consists of a single integer. Process zero reads the data from the user.\n",
    "![](../data/ring.gif)\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Send` `Recv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ksEl7NmeCIS9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksEl7NmeCIS9",
    "outputId": "ece02fe7-aa6f-4955-c7fa-fc869f0a2734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ring.py\n"
     ]
    }
   ],
   "source": [
    "%%file ring.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "if RANK == 0 :\n",
    "    print()\n",
    "    sendbuf = int(input()) \n",
    "    COMM.send(sendbuf, dest = RANK+1)\n",
    "\n",
    "if RANK != 0:\n",
    "    recvbuf = COMM.recv(source=RANK-1)\n",
    "    print(\"Process : \", RANK, \"obtain \",recvbuf, \"from \",RANK-1)\n",
    "\n",
    "    if RANK < SIZE - 1:\n",
    "        COMM.send(recvbuf, dest = RANK+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nxuSIQ3mRZOp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxuSIQ3mRZOp",
    "outputId": "252e3c03-48e7-443f-9f07-7dfc5a47c107"
   },
   "outputs": [],
   "source": [
    "# enter command for compile and run the program\n",
    "# ! mpirun -n 4  python ring.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zw585oKN2VMK",
   "metadata": {
    "id": "zw585oKN2VMK"
   },
   "source": [
    "## Exercise 4 Matrix vector product\n",
    "\n",
    "1. Use the `MatrixVectorMult.py` file to implement the MPI version of matrix vector multiplication.\n",
    "2. Process 0 compares the result with the `dot` product.\n",
    "3. Plot the scalability of your implementation. \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "CPU time of parallel multiplication using 2 processes is  174.923446\n",
    "The error comparing to the dot product is : 1.4210854715202004e-14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40AUEaYn2VML",
   "metadata": {
    "id": "40AUEaYn2VML",
    "outputId": "9b8cef42-0d71-4f12-bf5b-f67543cb5141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MatrixVectorMult_V0.py\n"
     ]
    }
   ],
   "source": [
    "%%file MatrixVectorMult_V0.py\n",
    "import time \n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "from numba import njit\n",
    "from mpi4py import MPI\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "seed(42)\n",
    "\n",
    "# function générique pour le produit matrice vecteur\n",
    "def matVectMult(A, b, C):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    for i in range(row):\n",
    "        a = A[i]\n",
    "        for j in range(col):\n",
    "            C[i] += a[j] * b[j]\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "# créons la matrice A et le vecteur b\n",
    "SIZE = 1000\n",
    "local_size = SIZE // nbproc\n",
    "\n",
    "\n",
    "# counts est la liste contenant les blocks de chaque  proc\n",
    "proc_block = local_size * SIZE\n",
    "counts =  [proc_block for i in range(nbproc)]\n",
    "\n",
    "if RANK == 0:\n",
    "    A = lil_matrix((SIZE, SIZE))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(SIZE))\n",
    "    A = A.toarray()\n",
    "    b = rand(SIZE)\n",
    "else :\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "## on envoie une copie de b sur chaque proc et on distribue une partie de  A \n",
    "## à chaque processus\n",
    "\n",
    "localMatrix = np.empty((local_size, SIZE), dtype = np.float64)\n",
    "b = COMM.bcast(b, root = 0)\n",
    "\n",
    "COMM.Scatterv([A, counts, MPI.DOUBLE], localMatrix, root = 0)\n",
    "\n",
    "## Au niveau de chaque processus on fait un produit entre la matrice locale et b\n",
    "localC = np.zeros(local_size)\n",
    "start = MPI.Wtime()\n",
    "matVectMult(localMatrix, b, localC)\n",
    "stop = MPI.Wtime()\n",
    "if RANK == 0:\n",
    "    print(\"\\n\\n CPU time of parallel multiplication using\", nbproc,\"processes is \", (stop - start)*1000)\n",
    "\n",
    "\n",
    "## On rassemble maintenant les résultats obtenus au niveau de chaque processus\n",
    "\n",
    "sendcounts = [local_size for i in range(nbproc)] \n",
    "if RANK == 0: \n",
    "    C = np.empty(SIZE, dtype = np.float64)\n",
    "else :\n",
    "    C = None\n",
    "\n",
    "# rassemblement des résultats dans C\n",
    "COMM.Gatherv(localC,[C, sendcounts, MPI.DOUBLE], root = 0)\n",
    "\n",
    "if RANK == 0 :\n",
    "    C_ = A.dot(b)\n",
    "    print(\"The error comparing to the dot product is :\", np.max(C_ - C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "HZuvW1lo2VML",
   "metadata": {
    "id": "HZuvW1lo2VML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key\n",
      "\n",
      " CPU time of parallel multiplication using 2 processes is  174.909336\n",
      "The error comparing to the dot product is : 1.4210854715202004e-14\n"
     ]
    }
   ],
   "source": [
    "# enter command for compile and run the program\n",
    "!mpirun -n 2 python3 MatrixVectorMult_V0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PN8jkFNc2VMM",
   "metadata": {
    "id": "PN8jkFNc2VMM"
   },
   "source": [
    "## Exercise 5 Calculation of π (Monte Carlo)\n",
    "\n",
    "1. Use the `PiMonteCarlo.py` file to implement the calculation of PI using Monte Carlo.\n",
    "2. Process 0 prints the result.\n",
    "3. Plot the scalability of your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Fw8wcPcv2VMN",
   "metadata": {
    "id": "Fw8wcPcv2VMN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PiMonteCarlo_V0.py\n"
     ]
    }
   ],
   "source": [
    "%%file PiMonteCarlo_V0.py\n",
    "# write your program here\n",
    "import random \n",
    "import timeit\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "INTERVAL = 1000 ** 2\n",
    "\n",
    "local_int = INTERVAL //nbproc \n",
    "random.seed(42)  \n",
    "\n",
    "def gen_points():\n",
    "     \n",
    "    \n",
    "    nbpoints= 0\n",
    "\n",
    "    \n",
    "    for i in range(local_int): \n",
    "      \n",
    "        # on choisit de générer les points x et y \n",
    "        # suivant une loi uniforme sur [0,2]\n",
    "                \n",
    "        x= random.uniform(0, 2) \n",
    "        y= random.uniform(0, 2) \n",
    "      \n",
    "        # Distance entre chaque point (x, y) et le centre du cercle O(1,1)\n",
    "        dist_centre= (x - 1)**2 + (y - 1)**2\n",
    "      \n",
    "        # on vérifie si (x, y) est à l'intérieur du cercle \n",
    "        if dist_centre<= 1: \n",
    "            nbpoints+= 1\n",
    "    \n",
    "    return nbpoints\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "nb_points = gen_points()\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# on fait la somme de tous les points obtenus au niveau de chaque processus\n",
    "nb_points = COMM.reduce(nb_points, op = MPI.SUM, root = 0)\n",
    "if RANK == 0:\n",
    "    \n",
    "    pi = 4 * nb_points/ INTERVAL\n",
    "    print('\\n')\n",
    "    print(\"Circle points number :\",nb_points)\n",
    "    print(\"Final Estimation of Pi=\", pi, \"cpu time :\",(end-start) * 1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "w8lJT4FV2VMN",
   "metadata": {
    "id": "w8lJT4FV2VMN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key\n",
      "\n",
      "Circle points number : 785596\n",
      "Final Estimation of Pi= 3.142384 cpu time : 319.21276000502985\n"
     ]
    }
   ],
   "source": [
    "# enter command for compile and run the program\n",
    "!mpirun -n 2 python3 PiMonteCarlo_V0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982bb73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "mpi_assign.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
